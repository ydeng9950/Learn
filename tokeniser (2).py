# -*- coding: utf-8 -*-
"""tokeniser

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cuaq7njYUWHxLlyKZEWAaZEfv-1Bb7v5
"""

import re
from collections import Counter
from math import sqrt

class Tokeniser:
    def __init__(self):
        self.vocabulary = None
        self.token_counts = None

    # Split text based on whitespace and punctuations
    def tokenise_on_punctuation(self, text):
        return re.findall(r'\w+|[!"#%&\'()*+,-./:;?@[\\]_{}¡§«¶·»¿‘’“”–—]', text)

    # Take the input as corpus
    def train(self, text):
        tokens = self.tokenise_on_punctuation(text)
        self.token_counts = Counter(tokens)
        self.vocabulary = set(tokens)

    # Raise "Runtime Error" with a massage if invoked before training
    def _evaluate_training(self):
        if self.vocabulary is None or self.token_counts is None:
            raise RuntimeError("The tokeniser has not been trained yet.")

    def tokenise(self, text, use_unk=False):

        self._evaluate_training()

        tokens = self.tokenise_on_punctuation(text)
        result = []
        for token in tokens:
            if token in self.vocabulary:
                result.append(token)
            elif use_unk:
                result.append("UNK")
            else:
                result.extend(token)
        return result

    # Filter tokens based on their occurrence counts in the corpus
    # Add a token to the output if it appeared threshold times or more
    # Check if use_unk is set to True.
    # Otherwise, it is split into individual characters
    def tokenise_with_count_threshold(self, text, threshold, use_unk=False):
        self._evaluate_training()
        tokens = self.tokenise_on_punctuation(text)
        result = []
        for token in tokens:
            if self.token_counts.get(token, 0) >= threshold:
                result.append(token)
            elif use_unk:
                result.append("UNK")
            else:
                result.extend(token)
        return result

    # Filter tokens by frequency
    # Retain tokens whose relative frequency in the data is greater than or equal to the specified threshold.
    # Check if use_unk is set to True.
    # Otherwise, it is split into individual characters
    def tokenise_with_freq_threshold(self, text, threshold, use_unk=False):
        self._evaluate_training()
        total_tokens = sum(self.token_counts.values())
        tokens = self.tokenise_on_punctuation(text)
        result = []
        for token in tokens:
            frequency = self.token_counts.get(token, 0) / total_tokens
            if frequency >= threshold:
                result.append(token)
            elif use_unk:
                result.append("UNK")
            else:
                result.extend(token)
        return result

# define a function that performs a statistical analysis of a tokenized corpus.
def get_stats(tokenised_corpus):
    type_count = len(set(tokenised_corpus))
    token_count = len(tokenised_corpus)
    type_token_ratio = type_count / token_count if token_count else 0

    token_lengths = [len(token) for token in tokenised_corpus]
    token_count_by_length = Counter(token_lengths)
    avg_token_length = sum(token_lengths) / token_count if token_count else 0

    variance = sum((length - avg_token_length) ** 2 for length in token_lengths) / token_count if token_count else 0
    token_length_std_dev = sqrt(variance)

    return {
        "type_count": type_count,
        "token_count": token_count,
        "type_token_ratio": type_token_ratio,
        "token_count_by_length": dict(token_count_by_length),
        "average_token_length": avg_token_length,
        "token_length_std": token_length_std_dev,
    }